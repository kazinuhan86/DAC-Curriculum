{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** \\\n",
    "Machine Learning focuses on building systems that can learn from and make decisions based on data. Instead of being explicitly programmed to perform a task, a machine learning model is trained on a dataset to identify patterns and make predictions or decisions without human intervention.\n",
    "\n",
    "**A general pipeline:** \\\n",
    "1.) Data Collection - Collecting data relevant to the problem you want to solve. \\\n",
    "2.) Training  - Using this dataset to train a model, which involves adjusting the model's parameters to minimize errors in its predictions. \\\n",
    "3.) Evaluation - Testing the model on new, unseen data to evaluate its performance. \\\n",
    "4.) Prediction + Deployment - Using the trained model to make predictions or decisions on new data, and deploying it so that other users may use it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate (Simple) Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate linear regression is a statistical method used to model the relationship between a single independent variable $x$ and a dependent variable $y$ by fitting a linear equation to observed data. It aims to predict the dependent variable based on the value of the independent variable.\n",
    "\n",
    "equation of line: $$y_i = wx_i + b + \\epsilon_i$$\n",
    "equation of your prediction: $$ \\hat{y_i} = wx_i + b$$\n",
    "\n",
    "How do you generate a line? You need a value for the Slope and Intercept. Use Least Squares method / Maximum Likelihood Estimation to determine. \n",
    "\n",
    "Objective: \\\n",
    "Minimise the sum of squared error terms ie: $$ \\min_{w,b} \\sum_i \\epsilon_i^2 = \\min_{w,b} \\sum_i (y_i - \\hat{y}_i)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identical to the univariate case except you're now modelling the relationship between $y$ with multiple other independent variables/features $x_1,x_2...x_n$.\n",
    "\n",
    "equation of line: $$y_i = w_1x_{1,i} + w_2x_{2,i} + ...  + b + \\epsilon_i$$\n",
    "equation of your prediction: $$ \\hat{y_i} = w_1x_{1,i} + w_2x_{2,i} + ... + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two metrics can be used **Mean Squared Error (MSE)** and **$R^2$ value**.\n",
    "\n",
    "We split our model into training data and validation/test data. Apply regression line fitted on train data into validation data to evaluate performance. MSE is pretty self explanatory, $R^2$ measures the proportion of variance of the dependent/target feature that is explained by the independent features. \n",
    "\n",
    "$$\n",
    " MSE = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2 \\\\\n",
    " R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n",
    "$$\n",
    "\n",
    "MSE is also known as the loss function - a function that maps events or values of variables onto a real number intuitively representing some \"cost\" associated with the event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting happens when your model fits the training data too well; performs worse on test data. \\\n",
    "Regularization tries to prevent this by adding a penalty term to a model's loss function.\n",
    "\n",
    "In Linear Regression:\n",
    "$$\n",
    "\\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2 + \\text{Regularization Term}\n",
    "$$\n",
    "Your left term reduces losses, right term prevents losses from decreasing excessively. Now let's see this in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import our libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = ax + b + Gaussian noise \n",
    "# def reg_data(a, b, n, s):\n",
    "#    rtn_x, rtn_y = [], []\n",
    "#    for i in range(n):\n",
    "#        x = np.random.normal(0.0, 0.5)\n",
    "#        y = a * x + b + np.random.normal(0.0, s)\n",
    "#        rtn_x.append(x) # input features\n",
    "#        rtn_y.append(y) # target values\n",
    "#    return np.array(rtn_x).reshape(-1,1), np.array(rtn_y)\n",
    "\n",
    "# # Generate 1,000 data points drawn from y = ax + b + noise\n",
    "# # s : standard deviation of the noise distribution\n",
    "# x, y = reg_data(a=0.5, b=0.3, n=1000, s=0.2)\n",
    "\n",
    "# # y = w0 + w1*x1 + w2*x2 + ... → w0*x0 + w1*x1 + w2*x2 + ... (x0 = 1)\n",
    "# # y = [w0, w1, w2, ...] * [x0, x1, x2, ...].T  (T : transpose)\n",
    "# # y = W * X.T\n",
    "# X = np.hstack([np.ones([x.shape[0], 1]), x]) # horizontally stack a column of ones (intercept) with input features\n",
    "# REG_CONST = 0.01   # regularization constant\n",
    "\n",
    "# # Loss function : Mean Squared Error\n",
    "# def ols_loss(W, args):\n",
    "#     e = np.dot(W, X.T) - y\n",
    "#     mse = np.mean(np.square(e))  # mean squared error\n",
    "#     loss = mse + REG_CONST * np.sum(np.square(W)) # this is Ridge (L2) Regularization\n",
    "    \n",
    "#     # save W and loss\n",
    "#     if args[0] == True:\n",
    "#         trace_W.append([W, loss])\n",
    "#     return loss\n",
    "\n",
    "# # Perform optimization process\n",
    "# trace_W = []\n",
    "# result = optimize.minimize(ols_loss, [-4., 4], args=[True]) # minimise loss function starting from initial weights [-4,4]\n",
    "# print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for more info on the output visit: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "# # x: is the vector of the optimal solution\n",
    "\n",
    "# # Plot the training data and draw the regression line.\n",
    "# y_hat = np.dot(result.x, X.T) # predicted values using the optimized weights and the design matrix\n",
    "# plt.figure(figsize=(6, 6)) \n",
    "# plt.scatter(x, y, s=5, c='r')\n",
    "# plt.plot(x, y_hat, c='blue')\n",
    "# plt.axvline(x=0, ls='--', lw=0.5, c='black')\n",
    "# plt.axhline(y=0, ls='--', lw=0.5, c='black')\n",
    "# plt.show()\n",
    "\n",
    "# # Draw the loss function and the path to the optimal point.\n",
    "# m = 5\n",
    "# t = 0.1\n",
    "# w0, w1 = np.meshgrid(np.arange(-m, m, t), np.arange(-m, m, t))\n",
    "# zs = np.array([ols_loss([a,b], [False]) for [a, b] in zip(np.ravel(w0), np.ravel(w1))])\n",
    "# z = zs.reshape(w0.shape)\n",
    "\n",
    "# fig = plt.figure(figsize=(7, 7))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Draw the surface of the loss function\n",
    "# ax.plot_surface(w0, w1, z, alpha=0.7)\n",
    "\n",
    "# # Draw the path to the optimal point.\n",
    "# b = np.array([tw0 for [tw0, tw1], td in trace_W])\n",
    "# w = np.array([tw1 for [tw0, tw1], td in trace_W])\n",
    "# d = np.array([td for [tw0, tw1], td in trace_W])\n",
    "# ax.plot(b, w, d, marker='o', color=\"r\")\n",
    "\n",
    "# ax.set_xlabel('W0 (bias)')\n",
    "# ax.set_ylabel('W1 (slope)')\n",
    "# ax.set_zlabel('distance')\n",
    "# ax.azim = -50\n",
    "# ax.elev = 50\n",
    "# plt.show()\n",
    "\n",
    "# # Check the R2 score\n",
    "# sst = np.sum(np.square(y - np.mean(y)))  # total sum of squares\n",
    "# sse = np.sum(np.square(y - y_hat))       # sum of squares of error\n",
    "# r2 = 1 - sse / sst\n",
    "# print('\\nR2 score = {:.4f}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Implementation in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling (Normalization/Standardization) is a technique that shifts data closer toward the origin and scales the different feature $x_i, x_j$ weights to ensure that they are not significantly different. If they are different, can also affect the estimation of our slope $w$ and intercept $b$. \\\n",
    "During regularization, it may also unfairly impose greater penalties on some coefficients over others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the Boston.csv dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read our data into a dataframe \n",
    "data = pd.read_csv(r\"C:\\Users\\kaz\\Desktop\\kaz\\DAC\\Boston (2).csv\")\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are what the column names represent:\n",
    "* CRIM - per capita crime rate by town\n",
    "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS - proportion of non-retail business acres per town.\n",
    "* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX - nitric oxides concentration (parts per 10 million)\n",
    "* RM - average number of rooms per dwelling\n",
    "* AGE - proportion of owner-occupied units built prior to 1940\n",
    "* DIS - weighted distances to five Boston employment centres\n",
    "* RAD - index of accessibility to radial highways\n",
    "* TAX - full-value property-tax rate per $10,000\n",
    "* PTRATIO - pupil-teacher ratio by town\n",
    "* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* LSTAT - % lower status of the population\n",
    "* MEDV - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     crim    zn  indus  chas    nox     rm   age     dis  rad  \\\n",
       "0           1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1   \n",
       "1           2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2   \n",
       "2           3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2   \n",
       "3           4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3   \n",
       "4           5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3   \n",
       "\n",
       "   tax  ptratio  lstat  medv  \n",
       "0  296     15.3   4.98  24.0  \n",
       "1  242     17.8   9.14  21.6  \n",
       "2  242     17.8   4.03  34.7  \n",
       "3  222     18.7   2.94  33.4  \n",
       "4  222     18.7   5.33  36.2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of the dataframe (try DataWrangler)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
       "Unnamed: 0                                                                     \n",
       "1           0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
       "2           0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
       "3           0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
       "4           0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
       "5           0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
       "\n",
       "            ptratio  lstat  price  \n",
       "Unnamed: 0                         \n",
       "1              15.3   4.98   24.0  \n",
       "2              17.8   9.14   21.6  \n",
       "3              17.8   4.03   34.7  \n",
       "4              18.7   2.94   33.4  \n",
       "5              18.7   5.33   36.2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the first column into the index\n",
    "data.set_index(data.columns[0], inplace=True)\n",
    "# alternatively, pd.read_csv('datasets/Boston.csv', index_col=0) can be used\n",
    "data.rename(columns={'medv':'price'}, inplace=True)\n",
    "# Rename the 'medv' column to 'price'\n",
    "\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features (x) and the target variable (y)\n",
    "\n",
    "y = data['price']\n",
    "x= data.drop(columns =['price'])\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y)\n",
    "\n",
    "# Initialize the scaler\n",
    "scalar = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "x_train_scaled  = scalar.fit_transform(x_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "x_test_scaled = scalar.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R2 (Linear Regression) = 0.724\n",
      "R2 (Ridge) = 0.724\n",
      "R2 (Lasso) = 0.726\n"
     ]
    }
   ],
   "source": [
    "# 1. LinearRegression() -> this applies mean centering internally to the data \n",
    "model = LinearRegression()\n",
    "model.fit(x_train_scaled, y_train)\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "# Visually check the predicted and actual y values ​​of the test data.\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(y_test, y_pred, s=20, c='r')\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_pred')\n",
    "plt.show()\n",
    "# Calculate the R2 score\n",
    "\n",
    "r2 = model.score(x_test_scaled, y_test)\n",
    "print('\\nR2 (Linear Regression) = {:.3f}'.format(r2))\n",
    "\n",
    "# 2. Ridge regularization\n",
    "model = Ridge(alpha=0.01)\n",
    "model.fit(x_train_scaled, y_train)\n",
    "r2 = model.score(x_test_scaled, y_test)\n",
    "print('R2 (Ridge) = {:.3f}'.format(r2))\n",
    "\n",
    "\n",
    "# 3. Lasso regularization\n",
    "model = Lasso(alpha=0.01)\n",
    "model.fit(x_train_scaled, y_train)\n",
    "r2 = model.score(x_test_scaled, y_test)\n",
    "print('R2 (Lasso) = {:.3f}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally Weighted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally Weighted Linear Regression (LWLR) is a non-parametric algorithm that fits multiple linear regressions to different subsets of the data, giving more weight to points closer to the target point. This allows the model to capture local patterns and variations in the data, making it highly flexible and adaptive to changes in the data distribution.\n",
    "\n",
    "Weighted Cost Function - calculate distance $d$ between test data point $px$ and all training data points, and calculate weight $w$ for each datapoint with a normal distribution for $d$. \n",
    "\n",
    "$$\n",
    "d_i = |px - x_i| \\\\\n",
    "    \n",
    "w_i = \\exp\\left(-\\frac{d^2}{2\\tau^2}\\right) \\quad \n",
    "    \\begin{cases}\n",
    "        d_i \\to 0 : w_i \\to 1 \\\\\n",
    "        d_i \\to \\infty : w_i \\to 0\n",
    "    \\end{cases} \\\\\n",
    "\n",
    "\\\\\n",
    "    \n",
    "\n",
    "\\min_{w,b} \\sum_i \\epsilon_i^2 = \\min_{w,b} \\sum_i w_i(y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tau$ is the standard deviation of the normal distribution and can adjust the range of neighbours; $\\tau$ is a hyperparameter.\n",
    "\n",
    "A hyperparameter is a parameter whose value is set before the learning process begins and controls the behavior of the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kaz\\Desktop\\kaz\\DAC\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Separate the features (x) and the target variable (y)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and test data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaz\\Desktop\\kaz\\DAC\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\kaz\\Desktop\\kaz\\DAC\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'"
     ]
    }
   ],
   "source": [
    "# Read our data into a dataframe \n",
    "data = pd.read_csv(r\"C:\\Users\\kaz\\Desktop\\kaz\\DAC\\Boston (2).csv\")\n",
    "data.shape\n",
    "\n",
    "# Separate the features (x) and the target variable (y)\n",
    "\n",
    "y = data['price']\n",
    "x = data.drop(columns=['price'])\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the training data and transform it\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "# Transform the test data using the same scaler\n",
    "x_test_scaled = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tx \u001b[38;5;129;01min\u001b[39;00m x_test_scaled:\n\u001b[1;32m----> 9\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[43mget_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50.0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# we set tau = 50.0\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(x_train_scaled, y_train, sample_weight \u001b[38;5;241m=\u001b[39m weight)\n",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m, in \u001b[0;36mget_weight\u001b[1;34m(train, test, tau)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_weight\u001b[39m(train, test, tau):\n\u001b[1;32m----> 3\u001b[0m     d2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39msquare(train \u001b[38;5;241m-\u001b[39m test), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m     w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39md2 \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2.\u001b[39m \u001b[38;5;241m*\u001b[39m tau \u001b[38;5;241m*\u001b[39m tau))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# train: training data, test: test data point to be predicted\n",
    "def get_weight(train, test, tau):\n",
    "    d2 = np.sum(np.square(train - test), axis=1)\n",
    "    w = np.exp(-d2 / (2. * tau * tau))\n",
    "    return w\n",
    "\n",
    "y_pred = []\n",
    "for tx in x_test_scaled:\n",
    "    weight = get_weight(x_train_scaled, tx, 50.0) # we set tau = 50.0\n",
    "    model = Ridge(alpha=0.01)\n",
    "    model.fit(x_train_scaled, y_train, sample_weight = weight)\n",
    "    y_pred.append(model.predict(tx.reshape(1, -1))[0])\n",
    "\n",
    "y_pred = np.array(y_pred).reshape(-1,)\n",
    "\n",
    "# we set tau = 50.0\n",
    "\n",
    "# Visually check the actual and predicted y values ​​of the test data.\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(y_test, y_pred, s=10, c='r')\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_pred')\n",
    "plt.show()\n",
    "\n",
    "print('\\nR2 (Locally Weighted Regression) = {:.3f}'.format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple (Binary) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical method for analyzing datasets in which there are one or more independent $y$ variables that determine an outcome, used for binary classification problems. It estimates the probability that a given input point belongs to a certain class using a logistic function.\n",
    "\n",
    "logistic function formula:\n",
    "$$\n",
    "\\hat{y}_i = \\frac{1}{1 + e^{-(wx_i + b)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Regression, we used Maxmimum Likelihood Estimation (MLE) to generate an objective function. In the same way, Logistic Regression can also use MLE to generate an objective function that minimises binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in breast cancer dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test data\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization constant (strength)\n",
    "REG_CONST = 0.01\n",
    "\n",
    "# Create a model and fit it to the training data.\n",
    "# C := inverse of regularization strength\n",
    "\n",
    "\n",
    "# Predict the classes of test data and measure the accuracy of test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "\n",
    "\n",
    "## Plot ROC curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "## Create and plot confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# # Print AUC score\n",
    "# print(f'AUC Score: {roc_auc:.3f}')\n",
    "# print(f'F1 Score: {f1:.3f}')\n",
    "\n",
    "# # Print Classification Report \n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Logistic Regression extends binary logistic regression to handle multiple classes by using techniques like one-vs-rest (OvR) or softmax regression. It estimates the probability of each class and assigns the input to the class with the highest probability.\n",
    "\n",
    "Here we will be looking at softmax regression. To obtain the loss function for softmax regression, we can use MLE and minimise cross entropy, which is a generalised form of binary cross entropy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in the iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have more things to process, let's break it down. We want to:\n",
    "\n",
    "1. Deal with categorical data \n",
    "2. Scale numeric values with a scaling function\n",
    "\n",
    "What other types of processes do we foresee having to do with our data? Hint: what about missing values? what about outliers?\n",
    "\n",
    "It would be tedious to go through all these processes manually - sklearn has a Pipeline class that simplifies these preprocessing/feature engineering steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and test data\n",
    "\n",
    "\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "\n",
    "# regularization constant (strength)\n",
    "REG_CONST = 0.01\n",
    "\n",
    "# Create a model and fit it to the training data.\n",
    "# C := inverse of regularization strength, stronger regularization with smaller values\n",
    "\n",
    "# Predict the classes of test data and measure the accuracy of test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get prediction probabilities\n",
    "# y_pred_proba = model.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "# # Create and plot confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "# print(f'F1 Score: {f1:.3f}')\n",
    "\n",
    "# # Print Classification Report \n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
